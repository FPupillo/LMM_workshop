---
title: "LMM tutorial"
author: "Francesco Pupillo"
date: "March, 2021"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 1
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Load packages
```{r load packages, message=FALSE, warning=FALSE, paged.print=FALSE}
# load the packages and source functions
library(dplyr)
library(ggplot2)
library(lme4)
library(lmerTest)
library(RLRsim)
source("helper_functions/simulateData.R")
# suppress scientific notation
options(scipen=5)
```

# Simulate data

```{r simulate Data}
# set.seed
set.seed(1234)
df<-my_sim_data(   
  SubN = 30,
  beta_0 = 0.60, # grand mean (fixed intercept)
  PEmean = 0.30,# PEmean
  beta_PE = 1.28, # effect of PE (fixed slope)
  n_items = 300, # number of trials
  tau_0 = 1.80, # by-subject random intercept sd .Check if it is sd, because than it will be squared into variance
  tau_1 = 2.50, # by-subject random slope sd
  rho = 0.3, # correlation between intercept and slope
  sigma = 0.3, # residual standard deviation
  RTorACC = 1 # Reaction times or accuracy (1 = RT, 2 = accuracy)
  )

# show first row of simulated data
head(df)
```
# Aggregate  

```{r aggregate}
# We are assuming that RT are normally distributed

# aggregate RT at the subject level
df_agg<-df %>%
  group_by(subj_id) %>%
  summarise(RT=mean(RT), PE=mean(PE)) 

# plot 
ggplot(df_agg, aes(y=RT, x = PE))+
  geom_point()+
  geom_smooth(method="lm")

# simple regression
linearmod<-lm(RT~PE, data = df_agg)

# get the summary
summary(linearmod)



```

# Unconditional model, random intercepts

```{r lmmr rand int}

# first, let's run an intercept only, unconditional model
mixmod_unc<-lmer(RT~1+(1|subj_id), data = df, control=lmerControl(optimizer="bobyqa"))#,optCtrl=list(maxfun=100000)))

summary(mixmod_unc)

# calculate the intraclass correlation
ICC<-VarCorr(mixmod_unc)$subj_id[1]/(VarCorr(mixmod_unc)$subj_id[1]+summary(mixmod_unc)$sigma^2)

# paste the results
paste("The IntraClass Correlation is",  ICC)

```

# Test significance of random intercepts

```{r rand int test sig}

# test significance
# model without random intercept. We can create a random intercept that is constant at 1
df$constint<-rep(1, nrow(df))

mod_unc<-lm(RT~1, data=df)
mixmod_unc<-lmer(RT~1+(1|subj_id), data = df)

anova(mixmod_unc, mod_unc)
```
# Simulation-based LRT

```{r sim based test}
# use simulation based-test
exactLRT(mixmod_unc,mod_unc)


```
# Maximal model

``` {r maximal}

# plot
  ggplot(df, aes(y=RT, x = PE))+
  geom_line(stat="smooth", method= "lm", formula = y~x, alpha = 0.5)+aes(colour = factor(subj_id))+
  geom_smooth(method="lm", colour="black")

maxMod<-lmer(RT~PE+(1+PE|subj_id), data = df, control=lmerControl(optimizer="bobyqa"))
summary(maxMod)

anova(maxMod)
```
# Centering


``` {r centering}

# grand mean centring and person mean centring
df<- df %>%
  # Grand mean centering (GMC)
  mutate (PE.gmc = PE-mean(PE)) %>%
  # Person mean centering (centering withing clusters - Participants)
  group_by(subj_id) %>%
  mutate(PE.cm = mean(PE),
         PE.cwc = PE-PE.cm ) %>%
  ungroup %>%
  # grand mean centering of the aggregated variable
  mutate(PE.cmc= PE.cm-mean(PE.cm))


maxModCent<-lmer(RT~PE.cwc+PE.cmc+(1+PE.cwc|subj_id), data = df, control=lmerControl(optimizer="bobyqa"))
summary(maxModCent)

```
# How much variance does our predictor explain on level 1 and 2

```{r Calculating R2pseudo between, within and between-within}

## R^2pseudo:within
# we create a new model including PE grand mean centered (PE_gmc) as preditor for RT
ModPE<-lmer(RT~PE.gmc+(1|subj_id), data = df, control=lmerControl(optimizer="bobyqa"))
summary(ModPE)

# used to quantify the change in (within) residual variance 
# variance residuum m0 - variance residuum m1 / variance residuum m0 = 1- Variance residuum m1/variance residuum m0
1 - summary(ModPE)$sigma^2/summary(mixmod_unc)$sigma^2

## R^2pseudo:between
# Variance Intercept modunc - Variance ModPE/Variance mixmod_unc = 1-Variance ModPE/Variance mixmod_unc
1 - VarCorr(ModPE)$subj_id[1]/VarCorr(mixmod_unc)$subj_id[1]

## R^2pseudo:bw (between and within)
# explained variance of each model = variance intercept + residual variance
# R^2pseudo:bw = 1-(Var.intercept ModPE + Var.residuum ModPE)/(Var. intercept mixedmod_unc + Var. Residuum mixedmod_unc)
1 - (VarCorr(ModPE)$subj_id[1] + summary(ModPE)$sigma^2)/(VarCorr(mixmod_unc)$subj_id[1] + summary(mixmod_unc)$sigma^2)

## if we want to include random slopes, we need to consider these sources of variances as well in our calculation

```

# Model selection backward
``` {r model selection backward}

# fit a model with covariance of random effects set at zero
maxModZeroCov<-lmer(RT~PE+PE+(PE||subj_id), data = df, control=lmerControl(optimizer="bobyqa"))

summary(maxModZeroCov)

# let's compare
anova(maxMod, maxModZeroCov )


```

# Excercise1

```{r  exercise1 model selection forward}

# baseline model
# m0 <- 

# random effects
# m1 <-

# make it more complex  
# m2 <-


```
# Categorical predictor between

``` {r categorical predictor between}

# create a categorical predictor between, simulating that we are randomly assigning participants
# to different groups of PE
PEbet<-rep(c("HighPE", "LowPE", "MediumPE"), each= nrow(df_agg)/3)

# created the levels in the aggregated dataset
  # pick random subject
set.seed(1234)
  df_agg$PEbw<-sample(PEbet, nrow(df_agg), replace = F)
  
  # we want it as factor
  df_agg$PEbw<-as.factor(df_agg$PEbw)

  # descriptives
df_agg %>%
  group_by(PEbw) %>%
  summarise(mean=mean(RT), sd = sd(RT))

# plot 
ggplot(df_agg, aes(PEbw, RT))+
  geom_bar(aes(PEbw, RT, fill = PEbw),
           position="dodge",stat="summary")+
  geom_point()+
  stat_summary(fun.data = "mean_cl_boot", size = 0.8, geom="errorbar", width=0.2 )# this line adds error bars


# create an Anova between participant
bwlm<-lm(RT~PEbw, data=df_agg)

summary(bwlm)

# re-level in order to have medium as the reference level
df_agg$PEbw<-relevel(df_agg$PEbw, ref = "MediumPE")

# refit
bwlm<-lm(RT~PEbw, data=df_agg)

summary(bwlm)

anova(bwlm)

```
# Setting contrasts

``` {r setting contrasts}
# method 1: contrast poly
contrasts(df_agg$PEbw)<-contr.poly(3) # we have three levels in our categorical predictor

# refit the model
bwlmContr<-lm(RT~PEbw, data=df_agg)

summary(bwlmContr)

# set our own contrasts
# linear
contrast1<-c(-1,0,1)
# quadratic
contrast2<-c(1,-2,1)


contrasts(df_agg$PEbw)<-cbind(contrast1, contrast2)
  
# refit the model
bwlmContrCust<-lm(RT~PEbw, data=df_agg)

summary(bwlmContrCust)               
          
```

# Continuous predictor between

``` {r continuous predictor between}

# what if we convert the categorical into continuous?
df_agg$PEbwC<-as.vector(NA)

for (n in 1:nrow(df_agg)){
  if (df_agg$PEbw[n]=="MediumPE"){
  df_agg$PEbwC[n]<-0.33
  } else if (df_agg$PEbw[n] == "HighPE"){
    df_agg$PEbwC[n]<-0.80
  }  else if (df_agg$PEbw[n] == "LowPE"){
    df_agg$PEbwC[n]<-0.20
  }
}

ggplot(df_agg, aes(y=RT, x = PEbwC))+
  geom_point()+
  geom_smooth(method="lm")

# fit lm
bwlmC<-lm(RT~PEbwC, data=df_agg)

summary(bwlmC)

```
# Categorical predictor within. EZ

``` {r categorical predictor within ez}

# we have a categorical variable in the dataset, which is PElevel
df$PElevel<-as.factor(df$PElevel)

# let's inspect that
  ggplot(df, aes(y=RT, x = PElevel))+
  geom_boxplot( )


# eazy anova on the aggregate dataset
df_aggbw<-df %>%
          group_by(subj_id,PElevel) %>%
          summarise(RT=mean(RT))

head(df_aggbw)

# ezanova
library(ez)
ezModel<-ezANOVA(data = df_aggbw, # dataframe
                 dv = .(RT), # dependent variable. This functions requires to place the name of each variable within .() 
                 wid = .(subj_id), # variable that identifies participants )
                 within = .(PElevel), # independent variable
                 detailed = T
                 )

ezModel



```
# Categorical predictor within: mixed-effects

``` {r categorical predictor within mixed-effects}

# 
ModCateg<-lmer(RT~PElevel+(PElevel|subj_id), data = df, control=lmerControl(optimizer="bobyqa"))
summary(ModCateg)

# try nlme
library(nlme)
test.lme<-lme(fixed = RT~PElevel, 
              random = ~ PElevel| subj_id, 
              data = df, 
              method = "REML", 
              control= lmeControl( opt = "optim", optimMethod ="BFGS" ))

summary(test.lme)
test.lme


```

# excercise2

```{r  exercise2 contrasts for within lmm}
# contr. poly

# custom contrast


```

# logistic regression simulate files
```{r log sim }
set.seed(1234)
df_acc<-my_sim_data(   
  SubN = 30,
  beta_0 = 0.60, # grand mean (fixed intercept)
  PEmean = 0.30,# PEmean
  beta_PE = 1.28, # effect of PE (fixed slope)
  n_items = 300, # number of trials
  tau_0 = 1.80, # by-subject random intercept sd .Check if it is sd, because than it will be squared into variance
  tau_1 = 2.50, # by-subject random slope sd
  rho = 0.3, # correlation between intercept and slope
  sigma = 0.2, # residual standard deviation
  RTorACC = 2 # Reaction times or accuracy (1 = RT, 2 = accuracy)
  )

# show first row of simulated data
head(df_acc)

```

```{r run logistic regression}

logmod<-glm(rec_acc~PE, data = df_acc, family = binomial)

summary(logmod)

# take the exponential of the coefficients to get the ODDS
OR <- exp(logmod$coefficients)

# calculate confidence intervals for the odds ratio
exp(confint(logmod))


```
```{r run generalised linear mixed-effects model}

# first, visualize the data
  ggplot(df_acc, aes(rec_acc, x = PE))+
  geom_line(stat="smooth", method= "glm", formula = y~x,method.args=list(family="binomial"), alpha = 0.5)+
  aes(colour = factor(subj_id))+
  geom_smooth(method="glm", method.args=list(family="binomial"),colour="black")

  
GLMMmod<-glmer(rec_acc~PE+(PE|subj_id), data = df_acc, family = binomial)

GLMMmodsum<-summary(GLMMmod)

GLMMmodsum

# take the exponential of the coefficients to get the ODDS
OR <- exp(GLMMmodsum$coefficients)

# calculate confidence intervals for the odds ratio
# Attention: it takes a long time to estimate!
# exp(confint(GLMMmod))

```

```{r run generalised linear mixed-effects model categorical}
# convert PElevel to a factor
df_acc$PElevel<-as.factor(df_acc$PElevel)

# plot 
ggplot(df_acc, aes(PElevel, rec_acc))+
  geom_bar(aes(PElevel, rec_acc, fill = PElevel),
           position="dodge",stat="summary")
#stat_summary(fun.data = "mean_cl_boot", size = 0.8, geom="errorbar", width=0.2 )# this line adds error bars

# model
GLMMmodCat<-glmer(rec_acc~PElevel+(PElevel|subj_id), data = df_acc, family = binomial)
summary(GLMMmodCat)

```

OR <- exp(fixef(GLMMmodCat))

# calculate confidence intervals for the odds ratio
exp(confint(GLMMmodCat))



``` {r parametric bootstrapping}
library(afex)

# Beware=: it takes time!
mixed(rec_acc~PElevel+(PElevel|subj_id),family = binomial,method = "PB",
      data=df_acc)
```

allFit(ModCateg)
ModCateg<-lmer(RT~PElevel+(PElevel|subj_id), data = df, control=lmerControl(optimizer="nmkbw",optCtrl=list(maxfun=1000000)))

df$PEleveSc<-scale(df$PElevel)

# Extra
# plot the regression line and the confidence intervals taking into account random effects
``` {r plot regression mixed-effects}
# get the packages for obtaining effects
library(effects)
# get the effects for PE, including confidence intervals
effect_PE<-effects::effect(term = "PE", mod=maxMod)
summary(effect_PE)

# as datafra,e
effect_PE<-as.data.frame(effect_PE)

ggplot()+ 
geom_line(data=df, aes(y=RT, x = PE, colour=factor(subj_id)),  stat="smooth", method= "lm", formula = y~x, alpha = 0.5)+
geom_line(data=effect_PE, aes(x = PE, y = fit))+
geom_ribbon(data=effect_PE, aes(x=PE, ymin = lower, ymax=upper), alpha=0.3)

```